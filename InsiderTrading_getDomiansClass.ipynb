{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"InsiderTrading_getDomiansClass.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1GN8ujRHUFRYWzFrI-SykNvReCD7nRZ-S","authorship_tag":"ABX9TyOG6pbc3lXNuGrrKI4hy0sq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"X-Yn1A2g6O14","colab_type":"code","colab":{}},"source":["#install required pandas SQL library\n","pip install pandasql"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e51Na_7y25PC","colab_type":"code","colab":{}},"source":["import glob #library to walk directory paths\n","import re #Regex library\n","import os    #library to join path names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnMlC7aO3H9r","colab_type":"code","colab":{}},"source":["leaker_path =  './drive/My Drive/DatosInsider/answers/r4.2-1'\n","thief_path = './drive/My Drive/DatosInsider/answers/r4.2-2'\n","sabotage_path = './drive/My Drive/DatosInsider/answers/r4.2-3'\n","act_path = './drive/My Drive/DatosInsider/r4.2/' #with activity info\n","dwh_path ='./drive/My Drive/DatosInsider/DWH_tables/' #final repository path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o8xBGCee0cbx","colab_type":"code","colab":{}},"source":["#Get all domains used by the identified insiders\n","\n","def get_insider_domains(path):\n","  all_files = glob.glob(path + \"/*.csv\")\n","  all_insider_domains=set([])\n","  #read all files in the path and separate into two kind of DF, one for http\n","  #and one for email  \n","  for filename in all_files:\n","    insider_file = open(filename, \"r\")    \n","    for line in insider_file:\n","      fields=line.split(\",\")   \n","      if fields[0]==\"http\":                  \n","        all_insider_domains.add(re.findall('^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)',fields[5])[0])            \n","      elif fields[0]== \"email\":\n","        for idx in range(5,9): \n","          if fields[idx] != '':    \n","            emails  =fields[idx].split(';')\n","            for email in emails:\n","                all_insider_domains.add(email.split('@')[1])           \n","                #all_insider_domains.add(fields[idx].split('@')[1])              \n","    insider_file.close()\n","    try:\n","      all_insider_domains.remove('dtaa.com')  #Domain of the same fictional company\n","    except:\n","      continue\n","  return all_insider_domains\n","\n","#Leaker files contain URL that insider used to shared information\n","all_leaker_domains=get_insider_domains(leaker_path)\n","\n","#Thief files contain the URL that the employees surfed looking for employment\n","# from a job websit or from the competitors\n","# also emails sent to the competitors\n","all_thief_domains=get_insider_domains(thief_path)\n","\n","#sabotage files include logs to malware or keylogger\n","all_sabotage_domains=get_insider_domains(sabotage_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbe8H2ox23Qp","colab_type":"code","outputId":"12ff8514-55d8-44d1-e24c-a6d8343c8bb9","executionInfo":{"status":"ok","timestamp":1584690872303,"user_tz":360,"elapsed":12,"user":{"displayName":"Jaime Ulises Jimï¿½nez Cardoso","photoUrl":"","userId":"16250515277039376212"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Get all domains for all users\n","\n","#http.csv\n","#* Fields: id, date, user, pc, url, content\n","http_file = open(os.path.join(act_path,'/http.csv'), \"r\")\n","all_users_domains=set([])\n","for line in http_file:\n","  fields=line.split(\",\")\n","  all_users_domains.add(re.findall('^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)',fields[4])[0])      \n","http_file.close()    \n","\n","#email.csv\n","#* Fields: id, date, user, pc, to, cc, bcc, from, size, attachment_count, content\n","email_file = open(os.path.join(act_path,'email.csv'), \"r\")\n","for line in email_file:\n","  fields=line.split(\",\")\n","  for idx in range(4,8): \n","    try:               \n","      emails  =fields[idx].split(';')\n","      for email in emails:\n","        all_users_domains.add(email.split('@')[1])\n","    except:\n","      continue\n","\n","email_file.close()   \n","\n","#Remove incorrect domain names\n","try:\n","  all_users_domains.remove('dtaa.com') #Domain of the same fictional company\n","  all_users_domains.remove(' ') \n","  all_users_domains.remove('') \n","except:\n","  print(\"Error\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Error\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fo0YElM3Js_h","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from pandasql import sqldf\n","\n","def add_domain_info(domain_df, bl_domains_df):  \n","  results=pd.merge(domain_df, bl_domains_df,left_on='domain', right_on='name', how='left')\n","  results.drop_duplicates('domain', keep='first',inplace=True)\n","  miss_domain=results[results.isnull().any(axis=1)]\n","  results=results.dropna()\n","  results.reset_index(drop=True, inplace=True)\n","  q = \"\"\"select ms.domain, bl.* from miss_domain ms left join bl_domains_df bl on bl.name like '%.' || ms.domain || '%' ;\"\"\"\n","  #pysqldf = lambda q: sqldf(q, locals())\n","  #df=pysqldf(q)\n","  df=sqldf(q, locals())\n","  df.drop_duplicates('domain', keep='first',inplace=True)\n","  results=results.append(df, ignore_index=True)\n","  return results\n","\n","#Load Black list domain from file\n","bl_domains=pd.read_csv(os.path.join(dwh_path,'BL_domains.csv'))\n","\n","all_leaker_domains= pd.DataFrame(all_leaker_domains, columns=['domain'])\n","all_leaker_domains= add_domain_info(all_leaker_domains,bl_domains)\n","all_leaker_domains.to_csv(os.path.join(dwh_path,'all_leaker_domains_classified.csv'), index=False)\n","\n","all_thief_domains= pd.DataFrame(all_thief_domains, columns=['domain'])\n","all_thief_domains= add_domain_info(all_thief_domains,bl_domains)\n","all_thief_domains.to_csv(os.path.join(dwh_path,'/all_thief_domains_classified.csv'), index=False)\n","\n","all_sabotage_domains= pd.DataFrame(all_sabotage_domains, columns=['domain'])\n","all_sabotage_domains=add_domain_info(all_sabotage_domains,bl_domains)\n","all_sabotage_domains.to_csv(os.path.join(dwh_path,'/all_sabotage_domains_classified.csv'), index=False)\n","\n","all_users_domains= pd.DataFrame(all_users_domains, columns=['domain'])\n","all_users_domains= add_domain_info(all_users_domains,bl_domains)\n","#all_users_domains.to_csv('./drive/My Drive/DatosInsider/DWH_tables/all_users_domains_classified.csv', index=False)\n","\n","#Final catalogue was manually updated by adding an extra column 'Competitor' were it was inferred from the data\n","# that planes and technology websites are competitors"],"execution_count":0,"outputs":[]}]}